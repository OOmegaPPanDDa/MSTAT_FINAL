#  塞選 answer 數在 100 筆以上的 question
data <- data %>%
group_by(question_title) %>%
mutate(ans_count = n()) %>%
ungroup() %>%
filter(ans_count > 400)
# 將 data 按 question_title 排列
# Reorder the data by question_title
data <- data[order(data$question_title),]
# 選擇某一問題
# (可以先 comment，到後面再選，只是程式負擔會比較大)
data <- subset(data, data$ans_upvote_num != 0)
# 隨機取一個問題
qid = sample(1:length(unique(data$question_title)),1)
#qid = 1
data <- data[data$question_title == unique(data$question_title)[qid],]
# 取得 stop_word
# Get stop words
# data$question_combined <- paste(data$question_title, data$question_detail)
# document <- c(unique(data$question_combined),unique(data$ans))
# stop_word <- get_stop_word(document)
# stop_word <- unique(c(stop_word, toTrad(stopwordsCN())))
stop_word <- readLines('all_stop_word.txt')
# 取得 回應時間
# Add response time
data$response_time <- time_transform(data)
# 取得 斷詞 vector
# 取得 斷詞後濾掉 stop_word 的 vector
# 取得 斷詞後濾掉 stop_word 的 character string
# Add segmented questions and answers
data$ans_seg_vec_with_stop <- sapply(data$ans, function(x) seg_worker[x])
data$ans_seg_vec <- sapply(data$ans_seg_vec_with_stop, function(x) filter_segment(x, stop_word))
data$ans_seg <- sapply(data$ans_seg_vec, function(x) paste(x, collapse = ' '))
#data$q_seg <- sapply(data$question_combined, function(x) paste(filter_segment(seg_worker[x], stop_word), collapse = ' '))
#取得字數、詞數、stop_word 數和 stop_word 比例
# Add number of characters, words, and percentage of stop words
data$n_char <- nchar(data$ans)
#data$n_word <- word_count(data)
data$n_word <- sapply(data$ans_seg_vec_with_stop, function(x) length(x))
data$n_stop <- sapply(data$ans_seg_vec_with_stop, function(x) sum(is.element(x, stop_word)))
data$per_stop <- data$n_stop/data$n_word
# 取得 情感分數
# Add sentiment score
data$senti_score <- senti(data)
# 取得 tfidf 文本相似度
# Add tf_idf_score for similarity
ans_tfidf <- by(data, data$question_title, tf_idf_score)
data$ans_tfidf <- unlist(ans_tfidf)
# 取得 文本分群 feature
# 有兩種分群一個是直接的 kmeans, 一個是 pca 降階後再 kmeans
# 取出來的 feature 可以把它想成該回答與各族群的接近度
# Add cluster score
ans_cluster <- by(data, data$question_title, get_cluster_feature)
shiny::runApp('dsR/dsr_zhihu/shiny_dsr_zhihu')
runApp('dsR/dsr_zhihu/shiny_dsr_zhihu')
library(readr)
train <- read_csv("~/Desktop/pecuD3_final/train.csv")
View(train)
unique(train$STOP_NAME)
stop_name<-unique(train$STOP_NAME)
paste(stop_name,'火車站', collapse = '')
paste0(stop_name,'火車站', collapse = '')
stop_df <- data.frame(stop_name = stop_name)
stop_df
stop_df <- stop_df %>% mutate(paste(stop_name,'火車站',collapse=''))
library(dplyr)
stop_df <- stop_df %>% mutate(paste(stop_name,'火車站',collapse=''))
stop_df
View(stop_df)
stop_df <- stop_df %>% mutate(stop_t_name = paste(stop_name,'火車站',collapse=''))
stop_df = stop_df[c(,1)]
stop_df = stop_df[c(1),]
stop_name
stop_df <- data.frame(stop_name = stop_name)
colnames(stop_df)
colnames(stop_df) <- '1'
stop_df <- stop_df %>% mutate(2 = paste(1,'火車站',collapse=''))
stop_df <- stop_df %>% mutate('2' = paste('1','火車站',collapse=''))
colnames(stop_df) = one
colnames(stop_df) = 'one'
stop_df <- stop_df %>% mutate(two = paste(one,'火車站',collapse=''))
stop_df <- stop_df %>% mutate(two = paste(stop_df$one,'火車站',collapse=''))
stop_name
for(name in stop_name){}
stop_name
stop_df <- data.frame(stop_name = stop_name)
for(i in 1:nrow(stop_df)){
stop_df$stop_name[i] = paste(stop_df$stop_name[i],' 火車站',collapse = '')
}
View(stop_df)
stop_df <- data.frame(stop_name = stop_name)
stop_df <- data.frame(sname = stop_name)
for(i in 1:nrow(stop_df)){
stop_df[i,sname] = paste(stop_df[i,sname],' 火車站',collapse = '')
}
for(i in 1:nrow(stop_df)){
stop_df[i,'sname'] = paste(stop_df[i,'sname'],' 火車站',collapse = '')
}
stop_name
to_add<-rep('火車站',length(stop_name))
to_add
paste(stop_name,to_add,collapse = '')
paste0(stop_name,to_add,collapse = '')
paste0(stop_name,to_add)
stop_name<-paste0(stop_name,to_add)
stop_name
write(stop_name,'stop_name.txt')
install.packages(library(googleVis))
install.packages('googleVis')
library(googleVis)
df=data.frame(country=c("US", "GB", "BR"),
val1=c(10,13,14),
val2=c(23,12,32))
Line <- gvisLineChart(df)
plot(Line)
Line
Bar <- gvisBarChart(df)
plot(Bar)
print(Bar, file="~/Desktop/AndrewGeoMap.js")
print(Bar, "char",file="~/Desktop/AndrewGeoMap.js")
print(Bar, "chart", file="~/Desktop/AndrewGeoMap.js")
source('~/.active-rstudio-document')
demo(googleVis)
require(datasets)
states
View(state.x77)
View(state.name)
state.name
library(ggmap) library(mapproj)
原文網址：https://read01.com/7Rdan.html
install.packages(ggmap)
install.packages('ggmap;)
install.packages('ggmap;)
;d
ssdsd(0)
install.packages('ggmap')
library(ggmpa)
library(ggmpap)
library(ggmap)
map <- get_map(location = 'Taiwan', zoom = 4)
ggmap(map)
library(mapproj)
ggmap(map)
install.packages('ggptoto')
install.packages('ggproto')
install.packages('ggmap')
install.packages("ggmap")
ggmap(map)
library(mapproj)
library(ggmap)
map <- get_map(location = 'Taiwan', zoom = 4)
ggmap(map)
map <- get_map(location = 'China', zoom = 4)
ggmap(map)
devtools::install_github("dkahle/ggmap")
devtools::install_github("hadley/ggplot2")
library(readr)
train_flow <- read_csv("~/D3/pecuD3_final/train_flow.csv")
View(train_flow)
library(dplyr)
train_simple <- subset(train,year==2005)
train_simple <- subset(train_flow,year==2005)
library(ggmap)
map <- get_map(location = "Taiwan", zoom = 8, language = "zh-TW", maptype = "roadmap")
ggmap(map, darken = c(0.5, "white")) +geom_point(aes(x = long, y = lat),
color = "red", data = train_simple)
is.na(train_simple)
sum(is.na(train_flow))
sum(is.nan(train_flow))
sum(is.na(train_flow))
ggmap(map)
+geom_point(aes(x = long, y = lat), size = 2, col="red",data = train_simple, alpha = 1)
+stat_density2d(data = train_simple, aes(x = long, y=lat,fill = ..level.., alpha = ..level..)
,size = 0.01, bins = 16, geom = "polygon")
+scale_fill_gradient(low = "green", high = "red",guide = FALSE)+scale_alpha(range = c(0, 0.3), guide = FALSE)
ggmap(map)
+geom_point(aes(x = long, y = lat), size = 2, col="red",data = train_simple, alpha = 1)
+stat_density2d(data = train_simple, aes(x = long, y=lat,fill = ..level.., alpha = ..level..)
,size = 0.01, bins = 16, geom = "polygon")
+scale_fill_gradient(low = "green", high = "red",guide = FALSE)+scale_alpha(range = c(0, 0.3), guide = FALSE)
ggmap(map)
+geom_point(aes(x = long, y = lat), size = 2, col="red",data = train_simple, alpha = 1)
+stat_density2d(data = train_simple, aes(x = long, y=lat,fill = ..level.., alpha = ..level..)
,size = 0.01, bins = 16, geom = "polygon")
+scale_fill_gradient(low = "green", high = "red",guide = FALSE)+scale_alpha(range = c(0, 0.3), guide = 'legend')
ggmap(map)
+geom_point(aes(x = long, y = lat), size = 2, col="red",data = train_simple, alpha = 1)
+stat_density2d(data = train_simple, aes(x = long, y=lat,fill = ..level.., alpha = ..level..)
,size = 0.01, bins = 16, geom = "polygon")
+scale_fill_gradient(low = "green", high = "red",guide = 'legend')+scale_alpha(range = c(0, 0.3), guide = F)
ggmap(map)
+geom_point(aes(x = long, y = lat), size = 2, col="red",data = train_simple, alpha = 1)
+stat_density2d(data = train_simple, aes(x = long, y=lat,fill = ..level.., alpha = ..level..)
,size = 0.01, bins = 16, geom = "polygon")
+scale_fill_gradient(low = "#132B43", high = "#56B1F7", space = "Lab", na.value = "grey50",
guide = "legend")+scale_alpha(range = c(0, 0.3), guide = F)
ggmap(map)
+geom_point(aes(x = long, y = lat), size = 2, col="red",data = train_simple, alpha = 1)
+stat_density2d(data = train_simple, aes(x = long, y=lat,fill = ..level.., alpha = ..level..)
,size = 0.01, bins = 16, geom = "polygon")
+scale_fill_gradient(,data=train_simple,low = "#132B43", high = "#56B1F7", space = "Lab", na.value = "grey50",
guide = "legend")+scale_alpha(range = c(0, 0.3), guide = F)
ggmap(map)
+geom_point(aes(x = long, y = lat), size = 2, col="red",data = train_simple, alpha = 1)
+stat_density2d(data = train_simple, aes(x = long, y=lat,fill = ..level.., alpha = ..level..)
,size = 0.01, bins = 16, geom = "polygon")
+scale_fill_gradient(data=train_simple,low = "#132B43", high = "#56B1F7", space = "Lab", na.value = "grey50",
guide = "legend")+scale_alpha(range = c(0, 0.3), guide = F)
ggmap(map)
+geom_point(aes(x = long, y = lat), size = 2, col="red",data = train_simple, alpha = 1)
+stat_density2d(data = train_simple, aes(x = long, y=lat,fill = ..level.., alpha = ..level..))
ggmap(map) +
geom_point(aes(x = lon, y = lat), size = 2, col="red",data = train_simple, alpha = 0.6) +
geom_density2d(data = train_simple, aes(x = lon, y=lat), size = 0.3)
ggmap(map) +
geom_point(aes(x = long, y = lat), size = 2, col="red",data = train_simple, alpha = 0.6) +
geom_density2d(data = train_simple, aes(x = long, y=lat), size = 0.3)
ggmap(map)
+geom_point(aes(x = long, y = lat), size = 2, col="red",data = train_simple, alpha = 1)+
stat_density2d(data = train_simple, aes(x = long, y=lat,fill = ..level.., alpha = ..level..)
,size = 0.01, bins = 16, geom = "polygon")+
scale_fill_gradient(low = "green", high = "red",guide = 'legend')+
scale_alpha(range = c(0, 0.3), guide = F)
ggmap(map)+geom_point(aes(x = long, y = lat), size = 2, col="red",data = train_simple, alpha = 1)+
stat_density2d(data = train_simple, aes(x = long, y=lat,fill = ..level.., alpha = ..level..)
,size = 0.01, bins = 16, geom = "polygon")+
scale_fill_gradient(low = "green", high = "red",guide = 'legend')+
scale_alpha(range = c(0, 0.3), guide = F)
train_simple <- subset(train_flow, year==2014)
View(train_simple)
train_simple %>% group_by(stop_name) %>% mutate(yearFLow = sum(flow))
train_simple %>% group(stop_name) %>% mutate(yearFLow = sum(flow))
train_simple %>% group_by(stop_name) %>% mutate(yearFLow = sum(flow)) %>% ungroup()
train_use <- train[,c('stop_name','yearFlow','lat','long')]
train_use <- train_simple[,c('stop_name','yearFlow','lat','long')]
train_use <- train_simple %>% group_by(stop_name) %>% mutate(yearFLow = sum(flow)) %>% ungroup()[,c('stop_name','yearFlow','lat','long')]
train_use <- train_simple %>% group_by(stop_name) %>% mutate(yearFLow = sum(flow)) %>% ungroup()
train_use <- train_use[,c('stop_name','yearFlow','lat','long')]
View(train_use)
train_use <- train_use[,c('stop_name','yearFLow','lat','long')]
colnames(train_use)[2] = 'yearFlow'
train_use <- unique(train_use)
train_use <- train_use %>% arrange(desc(yearFlow))
View(train_use)
ggmap(map)+geom_point(aes(x = long, y = lat), size = 2, col="red",data = train_use, alpha = 1)+
stat_density2d(data = train_use, aes(x = long, y=lat,fill = ..level.., alpha = ..level..)
,size = 0.01, bins = 16, geom = "polygon")+
scale_fill_gradient(low = "green", high = "red",guide = 'legend')+
scale_alpha(range = c(0, 0.3), guide = F)
ggmap(map)+geom_point(aes(x = long, y = lat), size = 2, col="red",data = train_use, alpha = 1)
write.csv(train_use,'2014_train_year_flow.csv',row.names = F)
View(train_simple)
View(train_use)
shiny::runApp('D3/pecuD3_final/pecuD3_shiny_final')
runApp('D3/pecuD3_final/pecuD3_shiny_final')
shiny::runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
install.packages('extrafont')
library(extrafont)
runApp('pecuR/shinyProject')
fonts()
fonttable()
library(extrafont)
fonttable()
fonttable()
fonts()
font_import()
font_import()
fonts()
fonttable()
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
shiny::runApp('dsR/dsr_zhihu/shiny_dsr_zhihu')
shiny::runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
shiny::runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
shiny::runApp('dsR/dsr_zhihu/shiny_dsr_zhihu')
shiny::runApp('pecuR/shinyProject_personal')
print(a)
source('~/Desktop/test.R')
library(readr)
res <- read_csv("~/ML2017/hw2/res.csv")
View(res)
library(readr)
res_prev <- read_csv("~/Desktop/res_prev.csv")
View(res_prev)
res$label
res_prev$label
res$label == res_prev$label
sum(res$label == res_prev$label)
library(readr)
prediction <- read_csv("~/Desktop/0.54786/prediction.csv")
View(prediction)
library(readr)
prediction_check <- read_csv("~/Desktop/0.54786/prediction_check.csv")
View(prediction_check)
c(1,2,3,4,5)==c(1,2,3,4,5)
c(1,2,3,4,'5 2')==c(1,2,3,4,'5 2')
c(1,2,3,4,'5 2')==c(1,2,3,4,'5 1')
prediction_check$tags == prediction$tags
sum(prediction_check$tags == prediction$tags) - nrow(prediction)
sum(prediction_check$tags == prediction$tags)
prediction_check$tags == prediction$tags
?rmna
?na.omit
na.omit(prediction_check$tags == prediction$tags)
!(na.omit(prediction_check$tags == prediction$tags))
sum(!(na.omit(prediction_check$tags == prediction$tags)))
source('~/MSTAT/MSTAT_FINAL/test_rf.R')
setwd("~/MSTAT/MSTAT_FINAL")
source('~/MSTAT/MSTAT_FINAL/test_rf.R')
View(bdata)
library(readr)
dataset <- read_csv(NULL)
View(dataset)
library(readr)
merge_data_2016 <- read_csv("~/MSTAT/MSTAT_FINAL/merge_data/merge_data_2016.csv")
View(merge_data_2016)
importane(brd)
importane(brf)
importance(brf)
importance(sort(brf))
importance(sorted(brf))
sort(importance(brf))
?cor
the_year = 2012
# load data
pre_year = the_year - 1
year = the_year
pre_year_path = paste0('./merge_data/merge_data_', pre_year, '.csv')
year_path = paste0('./merge_data/merge_data_', year, '.csv')
pre_year_data = read_csv(pre_year_path)
year_data = read_csv(year_path)
year_data <- year_data %>%
select(Player_Full_Name,Team,Salary)
names(year_data)[ncol(year_data)] = 'Salary_to_predict'
# merge year and pre_year data
# merge by Player_Full_Name
the_data <- merge(x = pre_year_data, y = year_data, by = c('Player_Full_Name')) %>%
arrange(desc(Salary_to_predict))
the_data <- the_data[,-which(names(the_data) %in% c('Team.y'))]
names(the_data)[3] = 'Team'
# remove duplicated
name_duplicated_data = the_data$Player_Full_Name[duplicated(the_data$Player_Full_Name)]
# print(name_duplicated_data)
the_data <- the_data %>%
filter(!(Player_Full_Name %in% name_duplicated_data))
year_record = rep(the_year, nrow(the_data))
the_data = cbind(year_record, the_data)
View(the_data)
names(the_data)
cor(the_data[,seq(from=8,to=92)],the_data[,94])
the_data[is.na(the_data)] <- -1
cor(the_data[,seq(from=8,to=92)],the_data[,94])
the_data = subset(the_data,Pos2!='P')
the_data = subset(the_data,Pos2!='C')
View(the_data)
cor(the_data[,seq(from=8,to=92)],the_data[,94])
the_data = subset(the_data,Pos2!='P')
the_data = subset(the_data,Pos2!='C')
cor(the_data[,seq(from=8,to=92)],the_data[,94])
the_year = 2015
# load data
pre_year = the_year - 1
year = the_year
pre_year_path = paste0('./merge_data/merge_data_', pre_year, '.csv')
year_path = paste0('./merge_data/merge_data_', year, '.csv')
pre_year_data = read_csv(pre_year_path)
year_data = read_csv(year_path)
year_data <- year_data %>%
select(Player_Full_Name,Team,Salary)
names(year_data)[ncol(year_data)] = 'Salary_to_predict'
# merge year and pre_year data
# merge by Player_Full_Name
the_data <- merge(x = pre_year_data, y = year_data, by = c('Player_Full_Name')) %>%
arrange(desc(Salary_to_predict))
the_data <- the_data[,-which(names(the_data) %in% c('Team.y'))]
names(the_data)[3] = 'Team'
# remove duplicated
name_duplicated_data = the_data$Player_Full_Name[duplicated(the_data$Player_Full_Name)]
# print(name_duplicated_data)
the_data <- the_data %>%
filter(!(Player_Full_Name %in% name_duplicated_data))
year_record = rep(the_year, nrow(the_data))
the_data = cbind(year_record, the_data)
the_data = subset(the_data,Pos2!='P')
the_data = subset(the_data,Pos2!='C')
the_data[is.na(the_data)] <- -1
cor(the_data[,seq(from=8,to=92)],the_data[,94])
the_year = 2012
# load data
pre_year = the_year - 1
year = the_year
pre_year_path = paste0('./merge_data/merge_data_', pre_year, '.csv')
year_path = paste0('./merge_data/merge_data_', year, '.csv')
pre_year_data = read_csv(pre_year_path)
year_data = read_csv(year_path)
year_data <- year_data %>%
select(Player_Full_Name,Team,Salary)
names(year_data)[ncol(year_data)] = 'Salary_to_predict'
# merge year and pre_year data
# merge by Player_Full_Name
the_data <- merge(x = pre_year_data, y = year_data, by = c('Player_Full_Name')) %>%
arrange(desc(Salary_to_predict))
the_data <- the_data[,-which(names(the_data) %in% c('Team.y'))]
names(the_data)[3] = 'Team'
# remove duplicated
name_duplicated_data = the_data$Player_Full_Name[duplicated(the_data$Player_Full_Name)]
# print(name_duplicated_data)
the_data <- the_data %>%
filter(!(Player_Full_Name %in% name_duplicated_data))
year_record = rep(the_year, nrow(the_data))
the_data = cbind(year_record, the_data)
the_data = subset(the_data,Pos2!='C')
the_data = subset(the_data,Pos2!='P')
the_data[is.na(the_data)] <- -1
cor(the_data[,seq(from=8,to=92)],the_data[,94])
the_data[,94]
names(the_data)[,94]
names(the_data)[94]
the_year = 2012
# load data
pre_year = the_year - 1
year = the_year
pre_year_path = paste0('./merge_data/merge_data_', pre_year, '.csv')
year_path = paste0('./merge_data/merge_data_', year, '.csv')
pre_year_data = read_csv(pre_year_path)
year_data = read_csv(year_path)
year_data <- year_data %>%
select(Player_Full_Name,Team,Salary)
names(year_data)[ncol(year_data)] = 'Salary_to_predict'
# merge year and pre_year data
# merge by Player_Full_Name
the_data <- merge(x = pre_year_data, y = year_data, by = c('Player_Full_Name')) %>%
arrange(desc(Salary_to_predict))
the_data <- the_data[,-which(names(the_data) %in% c('Team.y'))]
names(the_data)[3] = 'Team'
# remove duplicated
name_duplicated_data = the_data$Player_Full_Name[duplicated(the_data$Player_Full_Name)]
# print(name_duplicated_data)
the_data <- the_data %>%
filter(!(Player_Full_Name %in% name_duplicated_data))
year_record = rep(the_year, nrow(the_data))
the_data = cbind(year_record, the_data)
the_data = subset(the_data,Pos2!='P')
the_data = subset(the_data,Pos2!='C')
the_data[is.na(the_data)] <- -1
cor(the_data[,seq(from=8,to=92)],the_data[,94])
importance(brf)
print(brf)
importance(brf)
brf$importanceSD
brf$ntree
brf$inbag
brf$localImportance
brf$err.rate
brf$classes
brf$votes
source('~/MSTAT/MSTAT_FINAL/test_rf.R')
brf$forest
forest = brf$forest
forest$cutoff
forest$treemap
brf$
dd
brf
forest$xlevels
getTree(brf, 1, labelVar=TRUE)
View(getTree(brf, 1, labelVar=TRUE))
View(bdata)
View(bdata[,bdata$Player = 'Rodriguez, A'])
View(bdata[,bdata$Player == 'Rodriguez, A'])
bdata$Player == 'Rodriguez, A'
View(bdata[bdata$Player == 'Rodriguez, A'])
View(bdata[bdata$Player == 'Rodriguez, A',])
View(bdata[bdata$Player == 'Teixeira, M',])
View(bdata[bdata$Player == 'Teixeira, M',c('year_record','Player','Salary','Salary_to_predict')])
View(bdata[bdata$Player == 'Teixeira, M',c('year_record','Player','Salary','Salary_to_predict')])
View(bdata[bdata$Player == 'Suzuki, I',c('year_record','Player','Salary','Salary_to_predict')])
View(bdata[bdata$Player == 'Lee, C',c('year_record','Player','Salary','Salary_to_predict')])
View(bdata[bdata$Player == 'Hamilton, J',c('year_record','Player','Salary','Salary_to_predict')])
